{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77557f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 0. Cargar librerías ====\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from TheModel import build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3450013b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 modelos locales cargados.\n"
     ]
    }
   ],
   "source": [
    "# ==== 1. Cargar modelos locales ====\n",
    "import glob\n",
    "loaded_local_models = [tf.keras.models.load_model(f) for f in glob.glob(\"mnist_parte_*.keras\")]\n",
    "\n",
    "print(f\"{len(loaded_local_models)} modelos locales cargados.\")\n",
    "\n",
    "# Verificar arquitectura\n",
    "for i in range(len(loaded_local_models)-1):\n",
    "    assert loaded_local_models[i].summary() == loaded_local_models[i+1].summary(), \"¡Los modelos tienen arquitecturas distintas!\"\n",
    "\n",
    "# ==== 2. Cargar test completo ====\n",
    "(_, _), (x_test, y_test) = mnist.load_data()\n",
    "x_test = x_test / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28)\n",
    "\n",
    "# ==== 3. Obtener pesos de cada modelo ====\n",
    "local_weights = [model.get_weights() for model in loaded_local_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b04a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== MÉTODO 1: FedAvg ====\n",
    "# Promedio simple de pesos\n",
    "fedavg_weights = [np.mean(np.array(w), axis=0) for w in zip(*local_weights)]\n",
    "\n",
    "# ==== MÉTODO 2: FedMedian ====\n",
    "# Mediana de pesos por capa\n",
    "fedmedian_weights = [np.median(np.array(w), axis=0) for w in zip(*local_weights)]\n",
    "\n",
    "# ==== MÉTODO 3: FedWeightedAvg ====\n",
    "# Promedio ponderado según tamaño de datos de entrenamiento local\n",
    "local_sizes = []\n",
    "for i in range(1, 6):\n",
    "    d = np.load(f\"mnist_parte_{i}.npz\")\n",
    "    local_sizes.append(len(d['labels']))\n",
    "\n",
    "total_size = sum(local_sizes)\n",
    "weights_scaled = []\n",
    "for i, lw in enumerate(local_weights):\n",
    "    scale = local_sizes[i] / total_size\n",
    "    weights_scaled.append([w * scale for w in lw])\n",
    "\n",
    "fedweighted_weights = [np.sum(np.array(ws), axis=0) for ws in zip(*weights_scaled)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf6361e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 4. Función para evaluar un modelo global ====\n",
    "def evaluate_global_model(weights, name):\n",
    "    model = build.build_it()\n",
    "    model.set_weights(weights)\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    print(f\"=== Evaluación: {name} ===\")\n",
    "    print(classification_report(y_test, y_pred_classes))\n",
    "    model.save(f\"{name}.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b911df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 5. Evaluar cada estrategia de agregación ====\n",
    "evaluate_global_model(fedavg_weights, \"global_model_FedAvg\")\n",
    "evaluate_global_model(fedmedian_weights, \"global_model_FedMedian\")\n",
    "evaluate_global_model(fedweighted_weights, \"global_model_FedWeightedAvg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f15696b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 6. Para comparar: modelo centralizado ====\n",
    "# Cargar datos\n",
    "x_train_parts = []\n",
    "y_train_parts = []\n",
    "\n",
    "for i in range(1, 6):\n",
    "    d = np.load(f\"mnist_parte_{i}.npz\")\n",
    "    x_train_parts.append(d['images'])\n",
    "    y_train_parts.append(d['labels'])\n",
    "\n",
    "x_train = np.concatenate(x_train_parts) / 255.0\n",
    "y_train = np.concatenate(y_train_parts)\n",
    "x_train = x_train.reshape(-1, 28, 28)\n",
    "\n",
    "# Entrenamiento central\n",
    "model_central = build.build_it()\n",
    "history = model_central.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n",
    "model_central.save(\"global_model_Centralized.keras\")\n",
    "\n",
    "# Evaluación central\n",
    "y_pred = model_central.predict(x_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "print(\"=== Evaluación: Modelo Centralizado ===\")\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n",
    "# Graficar accuracy\n",
    "plt.plot(history.history['accuracy'], label='Accuracy Centralized')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy Centralized')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Centralized Training\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
